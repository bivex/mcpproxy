# Smart MCP Proxy â€” Design Document

## 1â€¯Overview

*Built on **FastMCP v2** for both its internal server runtime and as the client library to upstream MCP endpoints (ensuring dynamic tool registration and specâ€‘compliant notifications).*

The Smart MCP Proxy is a **federating gateway** that sits between an AI agent and any number of ModelÂ ContextÂ Protocol (MCP) servers. The proxy:

- **Discovers** tools exposed by upstream MCP servers (HTTP, SSE or stdio).
- **Indexes** their metadata using a configurable embedding backend (BM25, HuggingÂ Face local, or OpenAI).
- **Serves a single control tool `retrieve_tools`.** When the agent calls this tool, the proxy:
  1. Searches the index.
  2. **Automatically registers the topÂ 5 results** with its own FastMCP server instance.
  3. Emits `notifications/tools/list_changed` so connected clients immediately see the new tools, following MCP spec
- Persists tool metadata, SHAâ€‘256 hash and embedding reference in **SQLite +Â Faiss** for quick reload and change detection.

## 2â€¯Goals & Nonâ€‘Goals

|                                   | In Scope    | Out of Scope |
| --------------------------------- | ----------- | ------------ |
| Dynamic discovery of many MCP servers | âœ…           |              |
| Hybrid lexical / vector search        | âœ…           |              |
| Hotâ€‘reload of proxy without downtime  | ğŸš« (future) |              |
| Graphâ€‘based semantic reasoning        | ğŸš« (future) |              |

## 3â€¯Highâ€‘Level Architecture

```mermaid
graph TD
  subgraph Upstream MCP
    A1[gcore-mcp-server-http-prod]
    A2[gcore-docs]
    A3[gcore-mcp-server-with-oauth]
  end

  subgraph Proxy
    B1(ConfigÂ Loader) --> B2(IndexÂ Builder)
    B2 -->|vectors| B3(Faiss)
    B2 -->|metadata| B4(SQLite)
    B5(FastMCPÂ Server) -. list_changed .-> Agent
    B5 -- calls --> A1 & A2 & A3
  end

  Agent((AIÂ Agent))
  Agent -- retrieve_tool --> B5
  B5 -- new tool wrappers --> Agent
```

## 4â€¯Configuration

### 4.1â€¯Server list (Cursor IDEÂ style)

```json
{
  "mcpServers": {
    "gcore-mcp-server-http-prod":  {"url": "http://localhost:8081/mcp"},
    "gcore-docs":                  {"url": "http://localhost:8000/sse"},
    "gcore-mcp-server-with-oauth": {"url": "http://localhost:8080/mcp"},

    "gcore-mcp-server-prod": {
      "command": "uvx",
      "args": [
        "--from",
        "mcp-gcore-python@git+https://gitlab-ed7.cloud.gc.onl/algis.dumbris/mcp-gcore.git",
        "gcore-mcp-server"
      ],
      "env": {
        "GCORE_TOKEN": "${GCORE_TOKEN}",
        "PORT": "9090"
      }
    }
  }
}
```

### 4.2â€¯Embedding backend viaÂ ENV

| Variable         | Allowed values                                | Default |
| ---------------- | --------------------------------------------- | ------- |
| `SP_EMBEDDER`    | `BM25`, `HF`, `OPENAI`                        | `BM25`  |
| `SP_HF_MODEL`    | e.g. `sentence-transformers/all-MiniLM-L6-v2` | â€”       |
| `OPENAI_API_KEY` | your key                                      | â€”       |

The proxy chooses the search driver at startup; mixedâ€‘mode hybrid search (lexicalÂ + vector) is possible in future.

## 5â€¯SQLiteÂ +Â Faiss Schema

```sql
-- SQLite (file: proxy.db)
CREATE TABLE tools (
  id              INTEGER PRIMARY KEY,
  name            TEXT NOT NULL,
  description     TEXT,
  hash            TEXT NOT NULL,
  server_name     TEXT NOT NULL,
  faiss_vector_id INTEGER UNIQUE
);
CREATE INDEX idx_tools_hash ON tools(hash);
```

- **Vectors** are stored in a sideâ€‘car Faiss index (`tools.faiss`). `faiss_vector_id` provides the linkage.
- SHAâ€‘256Â hash =Â `sha256(name||description||params_json)` enables change detection.

## 6â€¯OperationalÂ Flow

| Phase            | Action                                                                                                                        |
| ---------------- | ----------------------------------------------------------------------------------------------------------------------------- |
| **Startâ€‘up**     | 1ï¸âƒ£Â Load JSON config â†’ 2ï¸âƒ£Â Fetch `tools/list` from each server â†’ 3ï¸âƒ£Â Insert/Update SQLite rows, embed & upsert Faiss vectors. |
| **User query**   | 4ï¸âƒ£Â Agent calls `retrieve_tool(query)` â†’ 5ï¸âƒ£Â Proxy scores candidates, picks topÂ 5, registers wrappers, fires `list_changed`.  |
| **Invocation**   | 6ï¸âƒ£Â Agent invokes newly appeared tools as normal MCP calls.                                                                   |
| **Refresh loop** | 7ï¸âƒ£Â Proxy polls upstream `notifications/tools/list_changed` (or periodic list) to maintain single source of truth.            |

## 7â€¯SecurityÂ &Â Rateâ€‘Limiting

- **OAuth**: servers marked with `oauth=true` in extended config use bearer tokens cached in memory.
- **Perâ€‘origin quotas**: simple tokenâ€‘bucket keyed by `server_name`.
- **Sandbox**: new wrappers execute via FastMCPâ€™s remote client; no Python eval happens inside the proxy.

## 8â€¯AlternativeÂ Designs

| Option                    | Pros                             | Cons                                    |
| ------------------------- | -------------------------------- | --------------------------------------- |
| **Remote pgvector DB**    | Horizontal scale, SQL queries    | Adds external dependency                |
| **Graph RAG (KG +Â HNSW)** | Captures interâ€‘tool dependencies | Higher complexity / writeâ€‘amplification |

## 9â€¯OpenÂ Questions

1. How much weight should synthetic questions (Ã Â la TDWA in ScaleMCP) have in embeddings vs. plain BM25?  
2. Should topâ€‘k be adaptive (e.g., scoreÂ â‰¥â€¯0.8) instead of a fixedÂ 5?  
3. Would batching multiple `retrieve_tools` calls per user request, as ScaleMCP does, significantly improve latency?  
